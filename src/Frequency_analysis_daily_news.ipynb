{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import time, random\n",
    "import pprint\n",
    "import urllib3\n",
    "import csv, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#리스트 및 변수 초기화\n",
    "nlist=[] \n",
    "List=[]\n",
    "mention = 0\n",
    "\n",
    "# 쿼리에 검색어 입력, 검색 시작날짜와 끝 날짜 지정\n",
    "query = \"문재인\"   # url 인코딩 에러는 encoding parse.quote(query) \n",
    "s_date = \"2018.08.11\" \n",
    "e_date = \"2018.09.14\" \n",
    "s_from = s_date.replace(\".\",\"\") \n",
    "e_to = e_date.replace(\".\",\"\") \n",
    "\n",
    "# 저장 엑셀 파일 지정\n",
    "f = open(\"C:/Users/user/Desktop/\" + query + '뉴스언급량0001.csv', 'w', encoding='cp949', newline='')\n",
    "wr = csv.writer(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(n_url): \n",
    "    global mention\n",
    "    news_detail = []\n",
    "    nlist.append(n_url) \n",
    "    breq = requests.get(n_url) \n",
    "    bsoup = BeautifulSoup(breq.content, 'html.parser') \n",
    "\n",
    "    # html 파싱 \n",
    "    title = bsoup.select('h3#articleTitle')[0].text \n",
    "    news_detail.append(title) \n",
    "\n",
    "    # 날짜 파싱\n",
    "    pdate = bsoup.select('.t11')[0].get_text()[:11] \n",
    "    news_detail.append(pdate)  \n",
    "    \n",
    "    # 기사 본문 크롤링 \n",
    "    _text = bsoup.select('#articleBodyContents')[0].get_text().replace('\\n', \" \") \n",
    "    btext = _text.replace(\"// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}\", \"\") \n",
    "    news_detail.append(btext.strip())\n",
    "    \n",
    "    if(query.count(\"+\") == 0):\n",
    "        for i in news_detail:\n",
    "            mention += i.count(query)\n",
    "    else:\n",
    "        m=query.split(\"+\")\n",
    "        for k in m:\n",
    "            for i in news_detail:\n",
    "                mention += i.count(k)\n",
    "    \n",
    "    return news_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018.08.11\n",
      "list index out of range\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from selenium.webdriver.chrome import options\n",
    "from selenium import webdriver\n",
    "\n",
    "options = options.Options()\n",
    "options.add_argument(r\"--headless\")\n",
    "n_date = s_date\n",
    "n_date_in = n_date.replace(\".\",\"\") \n",
    "num_pages = 1\n",
    "while int(n_date_in) < int(e_to):         \n",
    "    print(n_date)\n",
    "    num_pages = 1\n",
    "    while num_pages < 1000:\n",
    "        prepdate = n_date.replace(\".\",\"-\")\n",
    "        # 페이지 지정\n",
    "        #print(num_pages)\n",
    "        ua = str(UserAgent().random)\n",
    "        options.add_argument(r\"--{0}\".format(ua))\n",
    "        driver = webdriver.Chrome(r\"C:\\Users\\user\\Desktop\\chromedriver.exe\",\n",
    "                            chrome_options=options)\n",
    "        options.arguments.remove(r\"--{0}\".format(ua))\n",
    "        url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=2&ds=\" + n_date + \"&de=\" + n_date + \"&nso=so%3Ar%2Cp%3Afrom\" + n_date_in + \"to\" + n_date_in + \"%2Ca%3A&start=\" + str(num_pages) \n",
    "        #header 추가 \n",
    "        header = { \n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', \n",
    "        } \n",
    "        req = requests.get(url,headers=header) \n",
    "        cont = req.content \n",
    "        soup = BeautifulSoup(cont, 'html.parser')\n",
    "\n",
    "        for urls in soup.select(\"._sp_each_url\"): \n",
    "            try : \n",
    "                if urls[\"href\"].startswith(\"https://news.naver.com\"):\n",
    "                    get_news(urls[\"href\"]) \n",
    "                    news_detail = get_news(urls[\"href\"])\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                continue\n",
    "            except SSLError as e:\n",
    "                continue\n",
    "        num_pages = num_pages + 10\n",
    "    wr.writerow([n_date, mention])\n",
    "    if(int(n_date_in) == 20180831): n_date_in = \"20180901\"\n",
    "    else: n_date_in=str(int(n_date_in)+1)\n",
    "    n_date = n_date_in[:4] + '.' + n_date_in[4:6] + '.' + n_date_in[6:]\n",
    "f.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
